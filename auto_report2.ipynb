{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DateType, LongType, MapType, Row\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import Window\n",
    "from pyspark.rdd import RDD\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Auto Reportâ€“Spark\").getOrCreate()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([ \\\n",
    "    StructField('incident_id', IntegerType(), True), \n",
    "    StructField('incident_type', StringType(), True),\n",
    "    StructField('vin_num', StringType(), True),\n",
    "    StructField('make', StringType(), True),\n",
    "    StructField('model', StringType(), True),\n",
    "    StructField('year', StringType(), True),\n",
    "    StructField('incident_date', DateType(), True),\n",
    "    StructField('desc', StringType(), True)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+-----------------+--------+------+----+-------------+--------------------+\n",
      "|incident_id|incident_type|          vin_num|    make| model|year|incident_date|                desc|\n",
      "+-----------+-------------+-----------------+--------+------+----+-------------+--------------------+\n",
      "|          1|            I|VXIO456XLBB630221|  Nissan|Altima|2003|   2002-05-08|Initial sales fro...|\n",
      "|          2|            I|INU45KIOOPA343980|Mercedes|  C300|2015|   2014-01-01|Sold from EuroMotors|\n",
      "|          3|            A|VXIO456XLBB630221|    null|  null|null|   2014-07-02|   Head on collision|\n",
      "|          4|            R|VXIO456XLBB630221|    null|  null|null|   2014-08-05| Repair transmission|\n",
      "|          5|            I|VOME254OOXW344325|Mercedes|  E350|2015|   2014-02-01|    Sold from Carmax|\n",
      "|          6|            R|VOME254OOXW344325|    null|  null|null|   2015-02-06|Wheel allignment ...|\n",
      "|          7|            R|VXIO456XLBB630221|    null|  null|null|   2015-01-01|Replace right hea...|\n",
      "|          8|            I|EXOA00341AB123456|Mercedes| SL550|2016|   2015-01-01|   Sold from AceCars|\n",
      "|          9|            A|VOME254OOXW344325|    null|  null|null|   2015-10-01|      Side collision|\n",
      "|         10|            R|VOME254OOXW344325|    null|  null|null|   2015-09-01|       Changed tires|\n",
      "|         11|            R|EXOA00341AB123456|    null|  null|null|   2015-05-01|       Repair engine|\n",
      "|         12|            A|EXOA00341AB123456|    null|  null|null|   2015-05-03|    Vehicle rollover|\n",
      "|         13|            R|VOME254OOXW344325|    null|  null|null|   2015-09-01|Replace passenger...|\n",
      "|         14|            I|UXIA769ABCC447906|  Toyota|Camery|2017|   2016-05-08|Initial sales fro...|\n",
      "|         15|            R|UXIA769ABCC447906|    null|  null|null|   2020-01-02|Initial sales fro...|\n",
      "|         16|            A|INU45KIOOPA343980|    null|  null|null|   2020-05-01|      Side collision|\n",
      "+-----------+-------------+-----------------+--------+------+----+-------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.format(\"csv\") \\\n",
    "    .option(\"header\", False) \\\n",
    "    .schema(schema) \\\n",
    "    .load(\"/Users/mallory/Desktop/DataEngineering/Springboard/DistComp/hadoop_auto/data.csv\")\n",
    "df.show()\n",
    "#df.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1: Perform Map Operation \n",
    "Implement `extract_key_vin_value()` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_key_vin_value(x):\n",
    "    \"\"\":param x: data source loaded into SparkSession,\n",
    "        :output: dictionary tuple with mapping values to be transformed into MapType\"\"\"\n",
    "\n",
    "    vin_number = x.vin_num\n",
    "    make = x.make\n",
    "    year = x.year\n",
    "    model = x.model\n",
    "    incident_id = x.incident_id\n",
    "    incident_type = x.incident_type\n",
    "    incident_date = x.incident_date\n",
    "    desc = x.desc\n",
    "  \n",
    "    return (vin_number, {\"make\": make, \"year\": year, \"model\": model, \"incident_id\":incident_id, \"incident_type\":incident_type, \"incident_date\":incident_date, \"desc\":desc})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.PipelinedRDD"
      ]
     },
     "execution_count": 508,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vin_kv = df.rdd.map(lambda x: extract_key_vin_value(x))\n",
    "# vin_kv.cache()\n",
    "type(vin_kv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "# QA - was checking how to access each member ðŸ˜…\n",
    "# for key, value in vin_kv.collect():\n",
    "#     # print(key, value)\n",
    "#     print(key, value[\"make\"], value[\"year\"], value[\"model\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Perform Group Aggregation to Populate Make & Year to All Records\n",
    "Implement `populate_make()` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 638,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:5: SyntaxWarning: assertion is always true, perhaps remove parentheses?\n",
      "<>:5: SyntaxWarning: assertion is always true, perhaps remove parentheses?\n",
      "/var/folders/wh/97_1kpvs623dfzwvhvt5w6h80000gn/T/ipykernel_9630/3142899557.py:5: SyntaxWarning: assertion is always true, perhaps remove parentheses?\n",
      "  assert (isinstance(data_rdd, RDD), 'data_rdd is not an RDD')\n"
     ]
    }
   ],
   "source": [
    "# vin_kv.collectAsMap()\n",
    "sc = spark.sparkContext\n",
    "def populate_make(data_rdd):\n",
    "    # data = data_rdd.collect()\n",
    "    assert (isinstance(data_rdd, RDD), 'data_rdd is not an RDD')\n",
    "\n",
    "    output = []\n",
    "\n",
    "    for member in data_rdd:\n",
    "\n",
    "        if member[\"incident_id\"] != None:\n",
    "                incident_id = member[\"incident_id\"] # TODO TypeError: string indices must be integers\n",
    "        if member[\"incident_type\"] != None:\n",
    "                incident_type = member[\"incident_type\"]\n",
    "        if member[\"incident_date\"] != None:\n",
    "                incident_date = member[\"incident_date\"]\n",
    "        desc = member[\"desc\"]\n",
    "        if member[\"make\"] != None:\n",
    "                make = member[\"make\"]\n",
    "        if member[\"year\"] != None:\n",
    "                year = member[\"year\"]\n",
    "        if member[\"model\"] != None:\n",
    "                model = member[\"model\"]\n",
    "        output.append({key: {\"make\": make, \"year\": year, \"model\": model, \"incident_id\":incident_id, \"incident_type\":incident_type, \"incident_date\":incident_date, \"desc\": desc}})\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:7: SyntaxWarning: assertion is always true, perhaps remove parentheses?\n",
      "<>:7: SyntaxWarning: assertion is always true, perhaps remove parentheses?\n",
      "/var/folders/wh/97_1kpvs623dfzwvhvt5w6h80000gn/T/ipykernel_9630/3123430030.py:7: SyntaxWarning: assertion is always true, perhaps remove parentheses?\n",
      "  assert (isinstance(data_rdd, RDD), 'data_rdd is not an RDD')\n"
     ]
    }
   ],
   "source": [
    "# # vin_kv.collectAsMap()\n",
    "# sc = spark.sparkContext\n",
    "\n",
    "\n",
    "# def populate_make(data_rdd):\n",
    "#     # data = data_rdd.collect()\n",
    "#     assert (isinstance(data_rdd, RDD), 'data_rdd is not an RDD')\n",
    "\n",
    "#     output = []\n",
    "\n",
    "#     for member in data_rdd:\n",
    "#         value = member[1]\n",
    "#         key = member[0]\n",
    "        \n",
    "#         print(member[1]) # QA\n",
    "#         print(value) #QA\n",
    "\n",
    "#         incident_id = value[3] # TODO string index is out of range?\n",
    "#         incident_type = value[4]\n",
    "#         incident_date = value[5]\n",
    "#         desc = value[6]\n",
    "#         if value[0] != None:\n",
    "#             make = value[0]\n",
    "#         if value[1] != None:\n",
    "#             year = value[1]\n",
    "#         if value[2] != None:\n",
    "#             model = value[2]\n",
    "#         output.append({key: {\"make\": make, \"year\": year, \"model\": model, \"incident_id\": incident_id,\n",
    "#                       \"incident_type\": incident_type, \"incident_date\": incident_date, \"desc\": desc}})\n",
    "#     #return sc.parallelize(output)\n",
    "#     return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 585,
   "metadata": {},
   "outputs": [],
   "source": [
    "# QA\n",
    "#print(populate_make(vin_kv))\n",
    "\n",
    "# x = populate_make(vin_kv.collect())\n",
    "# for i in x.items():\n",
    "#      print(i)\n",
    "\n",
    "# type(x)\n",
    "#type(populate_make(vin_kv.collect()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # creating a bridge table to collect master information for each make and model\n",
    "# bridge_schema = StructType([\n",
    "#     StructField(\"vin_key\", StringType(), True),\n",
    "#     # use MapType to make use of key-value pairs returned by function\n",
    "#     StructField(\"properties\", MapType(StringType(), StringType(), True))\n",
    "# ])\n",
    "\n",
    "# bridge_df = spark.createDataFrame(data=vin_kv, schema=bridge_schema)\n",
    "# bridge_df.show(truncate=False)\n",
    "# # check schema\n",
    "# bridge_df.printSchema()\n",
    "\n",
    "# # options here were map_concat(), coalesce(), and explode()\n",
    "# # explode map column to create a new row for each element in the given map column\n",
    "# bridge_df = bridge_df.select(\"vin_key\", explode(\"properties\"))\n",
    "\n",
    "# bridge_df.printSchema()  # schema will validate the explosion\n",
    "# bridge_df.show()\n",
    "\n",
    "# # get distinct values & drop null to avoid duplication\n",
    "# df_distinct = bridge_df.select(\n",
    "#     \"vin_key\", \"key\", \"value\").distinct().na.drop().sort(\"key\")\n",
    "# df_distinct.show(truncate=False)\n",
    "\n",
    "# # get rid of null values to get year make and model as an array of maps\n",
    "# map_df = df_distinct.select(\"vin_key\", create_map(\"key\", \"value\").alias(\"map\")) \\\n",
    "#     .groupBy(\"vin_key\") \\\n",
    "#     .agg(collect_list(\"map\").alias(\"make_model_year\")) \\\n",
    "#     # .cache()\n",
    "\n",
    "# # make = map_df.select(\"vin_key\", map_df.make_model_year[0].alias(\"make\")).show()\n",
    "\n",
    "# # model = map_df.select(\"vin_key\", map_df.make_model_year[1].alias(\"model\")).show()\n",
    "\n",
    "# # year = map_df.select(\"vin_key\", map_df.make_model_year[2].alias(\"year\")).show()\n",
    "\n",
    "# # map_df.show(truncate=False)\n",
    "# # map_df.printSchema()\n",
    "\n",
    "# map_df = map_df.select(\"vin_key\", map_df.make_model_year[0].alias(\"make_map\"), map_df.make_model_year[1].alias(\n",
    "#     \"model_map\"), map_df.make_model_year[2].alias(\"year_map\"))  # .cache()\n",
    "\n",
    "# map_df.show()\n",
    "\n",
    "# map_df = map_df.select(\"vin_key\", map_df.make_map.getItem(\"make\").alias(\"make\"),\n",
    "#                        map_df.model_map.getItem(\"model\").alias(\"model\"),\n",
    "#                        map_df.year_map.getItem(\"year\").alias(\"year\")) \\\n",
    "#     .show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 639,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.rdd.PipelinedRDD'>\n",
      "[{'INU45KIOOPA343980': {'make': 'Nissan', 'year': '2003', 'model': 'Altima', 'incident_id': 1, 'incident_type': 'I', 'incident_date': datetime.date(2002, 5, 8), 'desc': 'Initial sales from TechMotors'}}, {'INU45KIOOPA343980': {'make': 'Nissan', 'year': '2003', 'model': 'Altima', 'incident_id': 3, 'incident_type': 'A', 'incident_date': datetime.date(2014, 7, 2), 'desc': 'Head on collision'}}, {'INU45KIOOPA343980': {'make': 'Nissan', 'year': '2003', 'model': 'Altima', 'incident_id': 4, 'incident_type': 'R', 'incident_date': datetime.date(2014, 8, 5), 'desc': 'Repair transmission'}}, {'INU45KIOOPA343980': {'make': 'Nissan', 'year': '2003', 'model': 'Altima', 'incident_id': 7, 'incident_type': 'R', 'incident_date': datetime.date(2015, 1, 1), 'desc': 'Replace right head light'}}, {'INU45KIOOPA343980': {'make': 'Mercedes', 'year': '2015', 'model': 'C300', 'incident_id': 2, 'incident_type': 'I', 'incident_date': datetime.date(2014, 1, 1), 'desc': 'Sold from EuroMotors'}}, {'INU45KIOOPA343980': {'make': 'Mercedes', 'year': '2015', 'model': 'C300', 'incident_id': 16, 'incident_type': 'A', 'incident_date': datetime.date(2020, 5, 1), 'desc': 'Side collision'}}, {'INU45KIOOPA343980': {'make': 'Mercedes', 'year': '2015', 'model': 'E350', 'incident_id': 5, 'incident_type': 'I', 'incident_date': datetime.date(2014, 2, 1), 'desc': 'Sold from Carmax'}}, {'INU45KIOOPA343980': {'make': 'Mercedes', 'year': '2015', 'model': 'E350', 'incident_id': 6, 'incident_type': 'R', 'incident_date': datetime.date(2015, 2, 6), 'desc': 'Wheel allignment service'}}, {'INU45KIOOPA343980': {'make': 'Mercedes', 'year': '2015', 'model': 'E350', 'incident_id': 9, 'incident_type': 'A', 'incident_date': datetime.date(2015, 10, 1), 'desc': 'Side collision'}}, {'INU45KIOOPA343980': {'make': 'Mercedes', 'year': '2015', 'model': 'E350', 'incident_id': 10, 'incident_type': 'R', 'incident_date': datetime.date(2015, 9, 1), 'desc': 'Changed tires'}}, {'INU45KIOOPA343980': {'make': 'Mercedes', 'year': '2015', 'model': 'E350', 'incident_id': 13, 'incident_type': 'R', 'incident_date': datetime.date(2015, 9, 1), 'desc': 'Replace passenger side door'}}, {'INU45KIOOPA343980': {'make': 'Mercedes', 'year': '2016', 'model': 'SL550', 'incident_id': 8, 'incident_type': 'I', 'incident_date': datetime.date(2015, 1, 1), 'desc': 'Sold from AceCars'}}, {'INU45KIOOPA343980': {'make': 'Mercedes', 'year': '2016', 'model': 'SL550', 'incident_id': 11, 'incident_type': 'R', 'incident_date': datetime.date(2015, 5, 1), 'desc': 'Repair engine'}}, {'INU45KIOOPA343980': {'make': 'Mercedes', 'year': '2016', 'model': 'SL550', 'incident_id': 12, 'incident_type': 'A', 'incident_date': datetime.date(2015, 5, 3), 'desc': 'Vehicle rollover'}}, {'INU45KIOOPA343980': {'make': 'Toyota', 'year': '2017', 'model': 'Camery', 'incident_id': 14, 'incident_type': 'I', 'incident_date': datetime.date(2016, 5, 8), 'desc': 'Initial sales from Carmax'}}, {'INU45KIOOPA343980': {'make': 'Toyota', 'year': '2017', 'model': 'Camery', 'incident_id': 15, 'incident_type': 'R', 'incident_date': datetime.date(2020, 1, 2), 'desc': 'Initial sales from Carmax'}}]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "enhance_make = vin_kv.groupByKey().flatMap(lambda kv: populate_make(kv[1]))\n",
    "print(type(enhance_make)) # RDD\n",
    "print(enhance_make.collect())  # list of dictionaries\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Count the # of Accident Occurrences for Each Vehicle Make & Year\n",
    "## 2.1 Perform Map Operation\n",
    "Implement `extract_key_make_value()` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 702,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def extract_key_make_value(data):\n",
    "#     # QA\n",
    "#     # assert (isinstance(data, dict), \"'data' from extract_key_make_value() must be a dictionary.\")\n",
    "#     # print(data)\n",
    "#     #print(type(data))\n",
    "    \n",
    "#     #data = data.collect() # returns list\n",
    "#     # iterate over data list\n",
    "#     for dict_item in data:\n",
    "#         # item should be a dictionary \n",
    "#         for value in dict_item.values():\n",
    "#             # print(values)\n",
    "#             if value[\"incident_type\"] == 'A':\n",
    "#                 count = ((value[\"make\"]+value[\"year\"]), 1)\n",
    "#                 print(type(count))\n",
    "#             else:\n",
    "#                 count = ((value[\"make\"]+value[\"year\"]), 0)\n",
    "#         # returns rdd\n",
    "#             return count \n",
    "\n",
    "\n",
    "# # QA\n",
    "# # extract_key_make_value(enhance_make)\n",
    "# enhance_make.map(lambda x: extract_key_make_value(x)).collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 715,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Nissan2003', 0),\n",
       " ('Nissan2003', 1),\n",
       " ('Nissan2003', 0),\n",
       " ('Nissan2003', 0),\n",
       " ('Mercedes2015', 0),\n",
       " ('Mercedes2015', 1),\n",
       " ('Mercedes2015', 0),\n",
       " ('Mercedes2015', 0),\n",
       " ('Mercedes2015', 1),\n",
       " ('Mercedes2015', 0),\n",
       " ('Mercedes2015', 0),\n",
       " ('Mercedes2016', 0),\n",
       " ('Mercedes2016', 0),\n",
       " ('Mercedes2016', 1),\n",
       " ('Toyota2017', 0),\n",
       " ('Toyota2017', 0)]"
      ]
     },
     "execution_count": 715,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def extract_key_make_value(data):\n",
    "    # QA\n",
    "    assert (isinstance(data, dict), \"'data' from extract_key_make_value() must be a dictionary.\")\n",
    "\n",
    "    # print(data)\n",
    "    for key, value in data.items():\n",
    "        # print(key, value)\n",
    "        if value[\"incident_type\"] == 'A':\n",
    "            count = ((value[\"make\"]+value[\"year\"]), 1)\n",
    "            # print(count)\n",
    "        else:\n",
    "            count = ((value[\"make\"]+value[\"year\"]), 0)\n",
    "        # returns rdd\n",
    "        return count\n",
    "\n",
    "\n",
    "# QA\n",
    "# extract_key_make_value(enhance_make)\n",
    "# enhance_make.map(lambda x: extract_key_make_value(x)).collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 719,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Nissan2003', 0), ('Nissan2003', 1), ('Nissan2003', 0), ('Nissan2003', 0), ('Mercedes2015', 0), ('Mercedes2015', 1), ('Mercedes2015', 0), ('Mercedes2015', 0), ('Mercedes2015', 1), ('Mercedes2015', 0), ('Mercedes2015', 0), ('Mercedes2016', 0), ('Mercedes2016', 0), ('Mercedes2016', 1), ('Toyota2017', 0), ('Toyota2017', 0)]\n"
     ]
    }
   ],
   "source": [
    "make_kv = enhance_make.map(lambda x: extract_key_make_value(x))\n",
    "\n",
    "# QA\n",
    "type(make_kv)\n",
    "print(make_kv.collect())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Aggregate Tuple Keys to Count Total Number of Records per Key Composite\n",
    "Use `reduceByKey()` to sum all values from each record."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 721,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.rdd.PipelinedRDD'>\n",
      "[('Nissan2003', 1), ('Mercedes2015', 2), ('Mercedes2016', 1), ('Toyota2017', 0)]\n"
     ]
    }
   ],
   "source": [
    "# using make_kv\n",
    "make_year_rdd = make_kv.reduceByKey(lambda key,count:key+count)\n",
    "\n",
    "# QA\n",
    "print(type(make_year_rdd))\n",
    "print(make_year_rdd.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 725,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------+\n",
      "|   make_year|accident_count|\n",
      "+------------+--------------+\n",
      "|  Nissan2003|             1|\n",
      "|Mercedes2015|             2|\n",
      "|Mercedes2016|             1|\n",
      "|  Toyota2017|             0|\n",
      "+------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_df = make_year_rdd.toDF(schema=[\"make_year\", \"accident_count\"])\n",
    "final_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 728,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.write.mode(\"append\").format(\"csv\").save(\"./report_csv\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2702f1c5dc7906726fa978e71b2cb3f3f6de4b4f62618e5f4fed2ff0a08c4bb4"
  },
  "kernelspec": {
   "display_name": "Python 3.9.4 64-bit ('env': venv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
