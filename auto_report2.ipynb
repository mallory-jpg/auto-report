{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/10/30 18:53:45 WARN Utils: Your hostname, Mallorys-MacBook-Pro.local resolves to a loopback address: 127.0.0.1; using 10.0.0.66 instead (on interface en0)\n",
      "21/10/30 18:53:45 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/Users/mallory/Desktop/DataEngineering/Springboard/DistComp/kafka-mini/env/lib/python3.9/site-packages/pyspark/jars/spark-unsafe_2.12-3.2.0.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "21/10/30 18:53:46 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "21/10/30 18:53:47 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "21/10/30 18:53:47 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DateType, LongType, MapType, Row\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import Window\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Auto Reportâ€“Spark\").getOrCreate()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pipe import chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([ \\\n",
    "    StructField('incident_id', IntegerType(), True), \n",
    "    StructField('incident_type', StringType(), True),\n",
    "    StructField('vin_num', StringType(), True),\n",
    "    StructField('make', StringType(), True),\n",
    "    StructField('model', StringType(), True),\n",
    "    StructField('year', StringType(), True),\n",
    "    StructField('incident_date', DateType(), True),\n",
    "    StructField('desc', StringType(), True)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+-----------------+--------+------+----+-------------+--------------------+\n",
      "|incident_id|incident_type|          vin_num|    make| model|year|incident_date|                desc|\n",
      "+-----------+-------------+-----------------+--------+------+----+-------------+--------------------+\n",
      "|          1|            I|VXIO456XLBB630221|  Nissan|Altima|2003|   2002-05-08|Initial sales fro...|\n",
      "|          2|            I|INU45KIOOPA343980|Mercedes|  C300|2015|   2014-01-01|Sold from EuroMotors|\n",
      "|          3|            A|VXIO456XLBB630221|    null|  null|null|   2014-07-02|   Head on collision|\n",
      "|          4|            R|VXIO456XLBB630221|    null|  null|null|   2014-08-05| Repair transmission|\n",
      "|          5|            I|VOME254OOXW344325|Mercedes|  E350|2015|   2014-02-01|    Sold from Carmax|\n",
      "|          6|            R|VOME254OOXW344325|    null|  null|null|   2015-02-06|Wheel allignment ...|\n",
      "|          7|            R|VXIO456XLBB630221|    null|  null|null|   2015-01-01|Replace right hea...|\n",
      "|          8|            I|EXOA00341AB123456|Mercedes| SL550|2016|   2015-01-01|   Sold from AceCars|\n",
      "|          9|            A|VOME254OOXW344325|    null|  null|null|   2015-10-01|      Side collision|\n",
      "|         10|            R|VOME254OOXW344325|    null|  null|null|   2015-09-01|       Changed tires|\n",
      "|         11|            R|EXOA00341AB123456|    null|  null|null|   2015-05-01|       Repair engine|\n",
      "|         12|            A|EXOA00341AB123456|    null|  null|null|   2015-05-03|    Vehicle rollover|\n",
      "|         13|            R|VOME254OOXW344325|    null|  null|null|   2015-09-01|Replace passenger...|\n",
      "|         14|            I|UXIA769ABCC447906|  Toyota|Camery|2017|   2016-05-08|Initial sales fro...|\n",
      "|         15|            R|UXIA769ABCC447906|    null|  null|null|   2020-01-02|Initial sales fro...|\n",
      "|         16|            A|INU45KIOOPA343980|    null|  null|null|   2020-05-01|      Side collision|\n",
      "+-----------+-------------+-----------------+--------+------+----+-------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.format(\"csv\") \\\n",
    "    .option(\"header\", False) \\\n",
    "    .schema(schema) \\\n",
    "    .load(\"/Users/mallory/Desktop/DataEngineering/Springboard/DistComp/hadoop_auto/data.csv\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_key_vin_value(x):\n",
    "    \"\"\":param x: data source loaded into SparkSession,\n",
    "        :output: dictionary tuple with mapping values to be transformed into MapType\"\"\"\n",
    "\n",
    "    vin_number = x.vin_num\n",
    "    make = x.make\n",
    "    year = x.year\n",
    "    model = x.model\n",
    "  \n",
    "    return (vin_number, {\"make\": make, \"year\": year, \"model\": model})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "vin_kv = df.rdd.map(lambda x: extract_key_vin_value(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+------------------------------------------------+\n",
      "|vin_key          |properties                                      |\n",
      "+-----------------+------------------------------------------------+\n",
      "|VXIO456XLBB630221|{model -> Altima, year -> 2003, make -> Nissan} |\n",
      "|INU45KIOOPA343980|{model -> C300, year -> 2015, make -> Mercedes} |\n",
      "|VXIO456XLBB630221|{model -> null, year -> null, make -> null}     |\n",
      "|VXIO456XLBB630221|{model -> null, year -> null, make -> null}     |\n",
      "|VOME254OOXW344325|{model -> E350, year -> 2015, make -> Mercedes} |\n",
      "|VOME254OOXW344325|{model -> null, year -> null, make -> null}     |\n",
      "|VXIO456XLBB630221|{model -> null, year -> null, make -> null}     |\n",
      "|EXOA00341AB123456|{model -> SL550, year -> 2016, make -> Mercedes}|\n",
      "|VOME254OOXW344325|{model -> null, year -> null, make -> null}     |\n",
      "|VOME254OOXW344325|{model -> null, year -> null, make -> null}     |\n",
      "|EXOA00341AB123456|{model -> null, year -> null, make -> null}     |\n",
      "|EXOA00341AB123456|{model -> null, year -> null, make -> null}     |\n",
      "|VOME254OOXW344325|{model -> null, year -> null, make -> null}     |\n",
      "|UXIA769ABCC447906|{model -> Camery, year -> 2017, make -> Toyota} |\n",
      "|UXIA769ABCC447906|{model -> null, year -> null, make -> null}     |\n",
      "|INU45KIOOPA343980|{model -> null, year -> null, make -> null}     |\n",
      "+-----------------+------------------------------------------------+\n",
      "\n",
      "root\n",
      " |-- vin_key: string (nullable = true)\n",
      " |-- properties: map (nullable = true)\n",
      " |    |-- key: string\n",
      " |    |-- value: string (valueContainsNull = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# creating a bridge table to collect master information for each make and model\n",
    "bridge_schema = StructType([\n",
    "    StructField(\"vin_key\", StringType(), True),\n",
    "    # use MapType to make use of key-value pairs returned by function\n",
    "    StructField(\"properties\", MapType(StringType(), StringType(), True))\n",
    "])\n",
    "\n",
    "bridge_df = spark.createDataFrame(data=vin_kv, schema=bridge_schema)\n",
    "bridge_df.show(truncate=False)\n",
    "# check schema\n",
    "bridge_df.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- vin_key: string (nullable = true)\n",
      " |-- key: string (nullable = false)\n",
      " |-- value: string (nullable = true)\n",
      "\n",
      "+-----------------+-----+--------+\n",
      "|          vin_key|  key|   value|\n",
      "+-----------------+-----+--------+\n",
      "|VXIO456XLBB630221|model|  Altima|\n",
      "|VXIO456XLBB630221| year|    2003|\n",
      "|VXIO456XLBB630221| make|  Nissan|\n",
      "|INU45KIOOPA343980|model|    C300|\n",
      "|INU45KIOOPA343980| year|    2015|\n",
      "|INU45KIOOPA343980| make|Mercedes|\n",
      "|VXIO456XLBB630221|model|    null|\n",
      "|VXIO456XLBB630221| year|    null|\n",
      "|VXIO456XLBB630221| make|    null|\n",
      "|VXIO456XLBB630221|model|    null|\n",
      "|VXIO456XLBB630221| year|    null|\n",
      "|VXIO456XLBB630221| make|    null|\n",
      "|VOME254OOXW344325|model|    E350|\n",
      "|VOME254OOXW344325| year|    2015|\n",
      "|VOME254OOXW344325| make|Mercedes|\n",
      "|VOME254OOXW344325|model|    null|\n",
      "|VOME254OOXW344325| year|    null|\n",
      "|VOME254OOXW344325| make|    null|\n",
      "|VXIO456XLBB630221|model|    null|\n",
      "|VXIO456XLBB630221| year|    null|\n",
      "+-----------------+-----+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# options here were map_concat(), coalesce(), and explode()\n",
    "# explode map column to create a new row for each element in the given map column\n",
    "bridge_df = bridge_df.select(\"vin_key\", explode(\"properties\"))\n",
    "\n",
    "bridge_df.printSchema()  # schema will validate the explosion\n",
    "bridge_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----+--------+\n",
      "|vin_key          |key  |value   |\n",
      "+-----------------+-----+--------+\n",
      "|UXIA769ABCC447906|make |Toyota  |\n",
      "|INU45KIOOPA343980|year |2015    |\n",
      "|VXIO456XLBB630221|model|Altima  |\n",
      "|EXOA00341AB123456|year |2016    |\n",
      "|VOME254OOXW344325|year |2015    |\n",
      "|UXIA769ABCC447906|year |2017    |\n",
      "|UXIA769ABCC447906|model|Camery  |\n",
      "|VOME254OOXW344325|model|E350    |\n",
      "|EXOA00341AB123456|model|SL550   |\n",
      "|VOME254OOXW344325|make |Mercedes|\n",
      "|VXIO456XLBB630221|make |Nissan  |\n",
      "|INU45KIOOPA343980|make |Mercedes|\n",
      "|INU45KIOOPA343980|model|C300    |\n",
      "|VXIO456XLBB630221|year |2003    |\n",
      "|EXOA00341AB123456|make |Mercedes|\n",
      "+-----------------+-----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# get distinct values & drop null to avoid duplication\n",
    "df_distinct = bridge_df.select(\"vin_key\", \"key\", \"value\").distinct().na.drop()\n",
    "df_distinct.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+------------------------------------------------------+\n",
      "|vin_key          |year_make_model                                       |\n",
      "+-----------------+------------------------------------------------------+\n",
      "|VOME254OOXW344325|[{year -> 2015}, {model -> E350}, {make -> Mercedes}] |\n",
      "|UXIA769ABCC447906|[{make -> Toyota}, {year -> 2017}, {model -> Camery}] |\n",
      "|VXIO456XLBB630221|[{model -> Altima}, {make -> Nissan}, {year -> 2003}] |\n",
      "|INU45KIOOPA343980|[{year -> 2015}, {make -> Mercedes}, {model -> C300}] |\n",
      "|EXOA00341AB123456|[{year -> 2016}, {model -> SL550}, {make -> Mercedes}]|\n",
      "+-----------------+------------------------------------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# get rid of null values to get year make and model\n",
    "map_df = df_distinct.select(\"vin_key\", create_map(\"key\", \"value\").alias(\"map\")) \\\n",
    "    .groupBy(\"vin_key\") \\\n",
    "    .agg(collect_list(\"map\").alias(\"year_make_model\"))\n",
    "\n",
    "map_df.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- vin_key: string (nullable = true)\n",
      " |-- year_make_model: array (nullable = false)\n",
      " |    |-- element: map (containsNull = false)\n",
      " |    |    |-- key: string\n",
      " |    |    |-- value: string (valueContainsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "map_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "cannot resolve 'concat_ws(',', year_make_model)' due to data type mismatch: argument 2 requires (array<string> or string) type, however, 'year_make_model' is of array<map<string,string>> type.;\n'Project [vin_key#122, concat_ws(,, year_make_model#240) AS year_make_model#267]\n+- Aggregate [vin_key#122], [vin_key#122, collect_list(map#234, 0, 0) AS year_make_model#240]\n   +- Project [vin_key#122, map(key#135, value#136) AS map#234]\n      +- Filter atleastnnonnulls(3, vin_key#122, key#135, value#136)\n         +- Deduplicate [vin_key#122, key#135, value#136]\n            +- Project [vin_key#122, key#135, value#136]\n               +- Project [vin_key#122, key#135, value#136]\n                  +- Generate explode(properties#123), false, [key#135, value#136]\n                     +- LogicalRDD [vin_key#122, properties#123], false\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/wh/97_1kpvs623dfzwvhvt5w6h80000gn/T/ipykernel_37965/688889249.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# df2 = df.withColumn(\"languagesAtSchool\",\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#                     concat_ws(\",\", col(\"languagesAtSchool\")))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmap_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"year_make_model\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconcat_ws\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\",\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"year_make_model\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Desktop/DataEngineering/Springboard/DistComp/kafka-mini/env/lib/python3.9/site-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mwithColumn\u001b[0;34m(self, colName, col)\u001b[0m\n\u001b[1;32m   2476\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mColumn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2477\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"col should be Column\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2478\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolName\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2479\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2480\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwithColumnRenamed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexisting\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/DataEngineering/Springboard/DistComp/kafka-mini/env/lib/python3.9/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1308\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1309\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1310\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1311\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/DataEngineering/Springboard/DistComp/kafka-mini/env/lib/python3.9/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: cannot resolve 'concat_ws(',', year_make_model)' due to data type mismatch: argument 2 requires (array<string> or string) type, however, 'year_make_model' is of array<map<string,string>> type.;\n'Project [vin_key#122, concat_ws(,, year_make_model#240) AS year_make_model#267]\n+- Aggregate [vin_key#122], [vin_key#122, collect_list(map#234, 0, 0) AS year_make_model#240]\n   +- Project [vin_key#122, map(key#135, value#136) AS map#234]\n      +- Filter atleastnnonnulls(3, vin_key#122, key#135, value#136)\n         +- Deduplicate [vin_key#122, key#135, value#136]\n            +- Project [vin_key#122, key#135, value#136]\n               +- Project [vin_key#122, key#135, value#136]\n                  +- Generate explode(properties#123), false, [key#135, value#136]\n                     +- LogicalRDD [vin_key#122, properties#123], false\n"
     ]
    }
   ],
   "source": [
    "# df2 = df.withColumn(\"languagesAtSchool\",\n",
    "#                     concat_ws(\",\", col(\"languagesAtSchool\")))\n",
    "df = map_df.withColumn(\"year_make_model\", concat_ws(\",\", col(\"year_make_model\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "cannot resolve 'element_at(year_make_model, 'year')' due to data type mismatch: Input to function element_at should have been array followed by a int, but it's [array<map<string,string>>, string].;\n'Project [unresolvedalias(element_at(year_make_model#240, year, false), Some(org.apache.spark.sql.Column$$Lambda$3471/0x0000000801135040@397ddc84))]\n+- Aggregate [vin_key#122], [vin_key#122, collect_list(map#234, 0, 0) AS year_make_model#240]\n   +- Project [vin_key#122, map(key#135, value#136) AS map#234]\n      +- Filter atleastnnonnulls(3, vin_key#122, key#135, value#136)\n         +- Deduplicate [vin_key#122, key#135, value#136]\n            +- Project [vin_key#122, key#135, value#136]\n               +- Project [vin_key#122, key#135, value#136]\n                  +- Generate explode(properties#123), false, [key#135, value#136]\n                     +- LogicalRDD [vin_key#122, properties#123], false\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/wh/97_1kpvs623dfzwvhvt5w6h80000gn/T/ipykernel_37965/3111904579.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# remove map from list format?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmap_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melement_at\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0myear_make_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"year\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Desktop/DataEngineering/Springboard/DistComp/kafka-mini/env/lib/python3.9/site-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mselect\u001b[0;34m(self, *cols)\u001b[0m\n\u001b[1;32m   1683\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Alice'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Bob'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1684\u001b[0m         \"\"\"\n\u001b[0;32m-> 1685\u001b[0;31m         \u001b[0mjdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jcols\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1686\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/DataEngineering/Springboard/DistComp/kafka-mini/env/lib/python3.9/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1308\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1309\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1310\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1311\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/DataEngineering/Springboard/DistComp/kafka-mini/env/lib/python3.9/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: cannot resolve 'element_at(year_make_model, 'year')' due to data type mismatch: Input to function element_at should have been array followed by a int, but it's [array<map<string,string>>, string].;\n'Project [unresolvedalias(element_at(year_make_model#240, year, false), Some(org.apache.spark.sql.Column$$Lambda$3471/0x0000000801135040@397ddc84))]\n+- Aggregate [vin_key#122], [vin_key#122, collect_list(map#234, 0, 0) AS year_make_model#240]\n   +- Project [vin_key#122, map(key#135, value#136) AS map#234]\n      +- Filter atleastnnonnulls(3, vin_key#122, key#135, value#136)\n         +- Deduplicate [vin_key#122, key#135, value#136]\n            +- Project [vin_key#122, key#135, value#136]\n               +- Project [vin_key#122, key#135, value#136]\n                  +- Generate explode(properties#123), false, [key#135, value#136]\n                     +- LogicalRDD [vin_key#122, properties#123], false\n"
     ]
    }
   ],
   "source": [
    "# remove map from list format?\n",
    "map_df.select(element_at(map_df.year_make_model, lit(\"year\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_df.withColumn(\"year\", map_df.year_make_model.get_item(\"year\")) \\\n",
    "    .withColumn(\"make\", map_df.year_make_model.get_item(\"model\")) \\\n",
    "    .withColumn(\"model\", map_df.year_make_model.get_item(\"year\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "data": {
      "text/plain": [
       "[('VXIO456XLBB630221', {'make': 'Nissan', 'year': '2003', 'model': 'Altima'}),\n",
       " ('INU45KIOOPA343980', {'make': 'Mercedes', 'year': '2015', 'model': 'C300'}),\n",
       " ('VXIO456XLBB630221', {'make': None, 'year': None, 'model': None}),\n",
       " ('VXIO456XLBB630221', {'make': None, 'year': None, 'model': None}),\n",
       " ('VOME254OOXW344325', {'make': 'Mercedes', 'year': '2015', 'model': 'E350'}),\n",
       " ('VOME254OOXW344325', {'make': None, 'year': None, 'model': None}),\n",
       " ('VXIO456XLBB630221', {'make': None, 'year': None, 'model': None}),\n",
       " ('EXOA00341AB123456', {'make': 'Mercedes', 'year': '2016', 'model': 'SL550'}),\n",
       " ('VOME254OOXW344325', {'make': None, 'year': None, 'model': None}),\n",
       " ('VOME254OOXW344325', {'make': None, 'year': None, 'model': None}),\n",
       " ('EXOA00341AB123456', {'make': None, 'year': None, 'model': None}),\n",
       " ('EXOA00341AB123456', {'make': None, 'year': None, 'model': None}),\n",
       " ('VOME254OOXW344325', {'make': None, 'year': None, 'model': None}),\n",
       " ('UXIA769ABCC447906', {'make': 'Toyota', 'year': '2017', 'model': 'Camery'}),\n",
       " ('UXIA769ABCC447906', {'make': None, 'year': None, 'model': None}),\n",
       " ('INU45KIOOPA343980', {'make': None, 'year': None, 'model': None})]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vin_kv.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def populate_make(df_with_nulls, map=map_df):\n",
    "    \"\"\"\n",
    "    Populate empty make values in dataset using map_df.\n",
    "    :param map: (optional) dataframe with mapped values to fill nulls\n",
    "    :param data_with_nulls: Spark rdd with null values to be replaced\n",
    "    :output: dataframe with filled null values \n",
    "    \"\"\"\n",
    "\n",
    "    # creating a bridge table to collect master information for each make and model\n",
    "    bridge_schema = StructType([\n",
    "        StructField(\"vin_key\", StringType(), True),\n",
    "        # use MapType to make use of key-value pairs returned by function\n",
    "        StructField(\"properties\", MapType(StringType(), StringType(), True))\n",
    "    ])\n",
    "\n",
    "    bridge_df = spark.createDataFrame(data=vin_kv, schema=bridge_schema)\n",
    "\n",
    "    # explode map column to create a new row for each element in the given map column\n",
    "    bridge_df = bridge_df.select(\"vin_key\", explode(\"properties\"))\n",
    "\n",
    "    # get distinct values & drop null to avoid duplication\n",
    "    df_distinct = bridge_df.select(\"vin_key\", \"key\", \"value\").distinct().na.drop()\n",
    "\n",
    "    # get rid of null values to get year make and model\n",
    "    map_df = df_distinct.select(\"vin_key\", create_map(\"key\", \"value\").alias(\"map\")) \\\n",
    "        .groupBy(\"vin_key\") \\\n",
    "        .agg(collect_list(\"map\").alias(\"year_make_model\"))\n",
    "    \n",
    "    \n",
    "\n",
    "    make = \n",
    "    model =\n",
    "    year = \n",
    "\n",
    "    return # values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "populate_make()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enhance_make = vin_kv.groupByKey().flatMap(lambda kv: populate_make(kv[1]))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2702f1c5dc7906726fa978e71b2cb3f3f6de4b4f62618e5f4fed2ff0a08c4bb4"
  },
  "kernelspec": {
   "display_name": "Python 3.9.4 64-bit ('env': venv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
